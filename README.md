# ðŸ“„ PDF Question Answering LLM (Ollama)

This project is a **Local LLM-powered Question Answering system** built with **Ollama**.  
It loads and processes hardcoded PDF files, and answers user questions based on the document content.

---

## ðŸš€ Overview
- Uses **Ollama** to run an LLM locally.
- Reads and extracts text from predefined PDF documents.
- Answers natural language questions by retrieving relevant context from the PDFs.
- Works fully offline once PDFs are loaded.

---

## ðŸ›  Tech Stack
- **Ollama** â€“ Local LLM runtime
- **Python / Node.js** (specify your implementation)
- **PDF Processing:** PyPDF2 / pdfplumber
- **Embedding & Retrieval:** In-memory or vector store (if used)

---

## ðŸ’¡ Key Features
- Local processing â€” no cloud API required.
- Accurate, context-aware answers from loaded PDFs.
- Simple architecture for quick prototyping.

---

## ðŸ“Œ Example Use Cases
- Offline document Q&A
- Internal knowledge base from PDFs
- Summarization of stored reports
